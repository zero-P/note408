## 项目相关

### 业务代码分层

<div align=left><img src="https://pic-zerooo.oss-cn-beijing.aliyuncs.com/ungee/1lm2Udv5UWGzez9.jepg.jpeg" width="400px"/></dev>


controller 层：MVC 中的 Controller 部分，是 web 应用的一个入口，这里面主要包含各种 controller 的定义。

facade 层（门面）：也是应用的入口，不过是 RPC 应用的入口，主要包含对外提供的 RPC 服务。

job 层：也是应用的入口，定时任务的调度入口，主要对外提供定时任务的调度服务。

listener 层：也是应用的入口，MQ 的消费入口，主要对外提供 MQ 消息的监听服务。

domain 层：领域层，把自己领域内的服务包装在一起。

application 层：应用层，复杂的编排各个领域服务。

infrastructure 层：基础设施层，主要是数据库访问、通用工具类、外部渠道对接等部分内容。

### 区块链

区块链是一种分布式数据库技术，它通过加密算法和共识机制，将数据以区块的形式链接在一起，形成一个不可篡改的、去中心化的数据链。

#### 核心概念

**<u>区块（Block）</u>**

区块是区块链的基本组成单位，每个区块包含一定数量的交易数据。

区块通常包括区块头（Block Header）和区块体（Block Body）。

+ 区块头包含元数据，如时间戳、前一个区块的哈希值（Previous Block Hash）、Merkle Root 等；

+ 区块体包含实际的交易数据。

**<u>链（Chain）</u>**

区块通过哈希值链接在一起，形成一条链。每个区块的区块头中包含前一个区块的哈希值，从而确保区块的顺序和完整性。

这种链式结构使得区块链具有不可篡改性，因为修改任何一个区块都会影响后续所有区块的哈希值。

**<u>去中心化（Decentralization）</u>**

区块链是一个分布式系统，数据存储在网络中的多个节点上，而不是集中在一个中心服务器上。

去中心化使得区块链具有更高的透明性和抗攻击性。

**<u>共识机制（Consensus Mechanism）</u>**

共识机制是区块链网络中节点之间达成一致的方法。

常见的共识机制包括工作量证明（Proof of Work, PoW）、权益证明（Proof of Stake, PoS）、委托权益证明（Delegated Proof of Stake, DPoS）等。

共识机制确保所有节点对区块链的状态达成一致，防止双重支付等恶意行为。

**<u>加密算法（Cryptography）</u>**

区块链使用加密算法确保数据的安全性和隐私性。

常见的加密算法包括哈希函数（如SHA-256）、非对称加密（如RSA、椭圆曲线加密）等。

加密算法还用于生成和验证数字签名，确保交易的真实性和完整性。

#### 区块链类型

**<u>公有链（Public Blockchain）</u>**

公有链是完全开放的，任何人都可以参与网络的维护和交易验证。比特币和以太坊（Ethereum）是典型的公有链。

**<u>联盟链（Consortium Blockchain）</u>**

联盟链是由多个组织共同维护的区块链，参与节点需要经过授权。联盟链在金融、供应链等领域有广泛应用。

**<u>私有链（Private Blockchain）</u>**

私有链是由单个组织控制的区块链，只有该组织的成员可以参与。私有链通常用于企业内部的数据管理和流程优化。

#### 区块链应用场景

**<u>加密货币</u>**

区块链最初的应用是加密货币，如比特币、以太坊等。加密货币通过区块链技术实现了去中心化的价值转移。

**<u>智能合约</u>**

智能合约是运行在区块链上的自动化程序，可以在满足特定条件时自动执行。以太坊是支持智能合约的典型平台。

#### Merkle Tree

默克尔树（又称哈希树）是一种二叉树结构，叶子节点存储数据块的哈希值，非叶子节点存储子节点哈希的组合哈希，根节点便是整个数据集的唯一指纹，写入区块头。

修改任一叶子节点会连锁改变所有父节点哈希，导致根哈希不匹配。验证某个数据是否在树中，轻节点只需从全节点该数据哈希和路径上的兄弟哈希，而无需下载整个数据集。

轻节点是区块链网络中的一种简化客户端，不存储完整的区块链数据，仅下载区块头（Block Header）和少量必要信息，依赖全节点进行交易验证。

全节点是区块链网络中的完整验证者，它不仅存储所有区块数据，还承担交易验证、共识规则执行等关键功能。

## 对公

### 负责网点待处理模块和小 H 开户模块开发以及后续全流程的日常维护

预审 → 尽职调查 → 一键打印

企业基本信息（预约编号、统一社会信用代码、企业名称）、账户关联人信息（法人、受益人）、预审信息展示（服务器以及三方校验结果、异常情况描述）

校验规则：关联人（年龄、国籍、身份证有效期）、企业（经营状态）

## 集作升级改造

### 负责统一远程调用包装工具设计与实现

服务调用方。

第一点，最基本，方便调用。

第二点，最重要，日志记录。

在我们的项目中，多个微服务之间需要通过 Dubbo 进行互相调用，在调用的时候，我们通常需要记录日志，尤其是调用方，方便我们排查问题，为了简化日志的记录，我们定义了一个通用的远程方法调用的包装工具类 RemoteCallWrapper，每次远程调用都是通过这个工具类，这样在每次外部调用的时候，可以把入参、出参，也就是请求报文、响应报文，以及耗时等信息打印到日志中，方便我们进行问题排查。

### 基于 AOP 和自定义注解，实现通用的 RPC 请求处理，用于通用的参数校验及响应封装

作为服务的提供方，我们也需要这些信息，因为有的时候我们做排查问题，我们总要知道别人调我是怎么调的，然后参数是怎么样的，我的返回值是怎么样的，这样方便我去排查问题是吧？方便也方便我去做一些日志，打点一些记录。我们在这个地方的话，为了方便大家去使用，然后我们自己定义的一个注解 Facade。

基于 aop 加自定义注解，实现了一个统一的 rpc 调用的被调用方的统一的处理：

+ 统一的参数校验。

  也就是说我作为一个 rpc 调用的提供者，一进来的时候，我就通过切面去做了一个拦截，先去做了一个统一的参数校验，校验一下我要求的你参数的格式，什么规定你有没有符合，如果不符合我就快速的给你返回失败了，也就是所谓的 fail fast，就不需要我再去做后续的操作了。

  利用切面的自定义注解和第三方参数校验框架比如 HibernateValidator 就可以很方便地去做统一的参数校验（@NotNull、@NotBlank、@Min），而不是每个服务方法都要各自去校验的话，这样就显得繁琐不优雅了。

+ 统一的异常捕获，确保我们的 RPC 接口不会返回异常，而是统一以错误码的形式返回。

+ 统一的日志的打印。

+ 统一的耗时的统计。

+ 将 response 的信息补全，主要是 code 和 message。

日志还可以帮我们后期去做一些监控，通过正则表达式去匹配一些字符，然后去做一个平均耗时的统计，以及接口调用的成功率等。

**<u>代理模式</u>**

给某一个对象提供一个代理，并由代理对象控制对原对象的引用。

一般是用代理对象去扩展目标对象的功能。

根据代理的创建时期，代理模式分为静态代理和动态代理

1. 静态：由程序员创建代理类或特定工具自动生成源代码再对其编译，在程序运行前代理类的 .class 文件就已经存在了。
2. 动态：在程序运行时，运用反射机制动态创建而成。

**<u>反射</u>**

反射机制允许我们在运行时分析类以及执行类中方法的能力。

- 在运行时获知任意一个对象所属的类
- 在运行时获知任意一个类所具有的成员变量和方法
- 在运行时构造任意一个类的对象
- 在运行时调用任意一个对象的方法和属性

原理：JVM 在运行时通过类的全限定名、实例的 getClass() 方法等获取 Class 对象信息，然后从堆中匹配到该类的 Class 对象，通过 Class 对象访问到方法区中类的类型、字段、方法信息、构造器等，反射机制可以把类中的这些成分映射成一个个对象，以进行进一步的使用。

显然，Class 对象是反射的基础。

优点：可以让代码更加灵活，为各种框架实现开箱即用的功能提供了便利。

缺点：

1. 安全问题：反射机制破坏了封装性，因为通过反射可以访问类的私有方法和字段，final 型字段也可再次修改，也能无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。
2. 性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的 Java 代码要慢很多。

**<u>AOP</u>**

Aspect oriented Programming，面向切面编程，能将那些与业务无关，却为业务模块所共同调用的逻辑（例如日志管理、资源释放、事务处理等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。

Spring AOP 默认使用动态代理（运行时）实现：

+ 如果目标对象实现了接口，使用 JDK 动态代理，基于反射机制，运行时动态创建接口的代理类
+ 如果目标对象没有实现接口，通过字节码技术生成目标类的子类。

AspectJ 属于静态织入，基于字节码操作，有一个专门的字节码生成器来生成 class 文件，织入时机有：编译期织入、编译后织入、类加载后织入。

Pointcut 可精确到方法，能设置多个 execution 表达式、自定义注解（指哪打哪）；切面类可在相对于切点时间可以做多种增强：Before、After、Around、AfterReturning、AfterThrowing，它们在切面类中以方法形式存在。这些增强方法可以传入参数 JoinPoint，通过它可以拿到代理类对象、委托类对象、委托类类名、切点方法名、切点方法参数等信息。

### 基于AOP+自定义注解+Redisson实现一套分布式锁注解，减少重复代码，降低出错

代码中要经常使用分布式锁，有很多锁，是要在方法入口处加锁，结束后解锁的，于是为了方便，定义了一个通用的注解，来进行分布式锁的实现。

@DistributeLock 属性有：

+ 锁的场景 `String scene();`
+ 加锁的 key `String key() default DistributeLockConstant.NONE_KEY;`
+ SPEL 表达式 `String keyExpression() default DistributeLockConstant.NONE_KEY;`
+ 过期时间 `int expireTime() default DistributeLockConstant.DEFAULT_EXPIRE_TIME;`
+ 等待时间 `int waitTime() default DistributeLockConstant.DEFAULT_WAIT_TIME;`

lockKey = scene + "#" + key，其中的 key 优先取注解属性 key，属性 key 没有设置的话就通过 SpEL 表达式动态地从方法参数中去取。

> SpEL 是 Spring 框架中非常强大的表达式语言，它提供了丰富的功能来处理各种表达式需求。通过 SpEL，开发者可以在运行时动态地访问和操作对象，极大地增强了 Spring 应用的灵活性和可配置性。

```java
@DistributeLock(keyExpression = "#telephone", scene = "USER_REGISTER")
@DistributeLock(keyExpression = "#request.identifier", scene = "ORDER_CREATE")
```

用到了 Redisson 的 RLock 进行了加锁，并且是用的 lock 方法，在加锁失败时阻塞一直尝试。

在这个切面上加了个 @Order(Integer.MIN_VALUE)，让它可以最早执行，避免并发时事务未提交但是锁释放导致的问题（进入事务 → 加锁 → 解锁 → 事务提交）。

### 基于 Sa-token 实现轻量级用户单点登录及鉴权，基于 Redis 实现分布式 Session

Sa-Token 是一个轻量级 Java 权限认证框架，主要解决：登录认证、权限认证、单点登录、分布式 Session 会话等一系列权限相关问题。

1. 路由拦截鉴权

   注册路由拦截器 Interceptor。

   ```java
   // 根据路由划分模块，不同模块不同鉴权 
   registry.addInterceptor(new SaInterceptor(handler -> {
       SaRouter.match("/user/**", r -> StpUtil.checkPermission("user"));
       SaRouter.match("/admin/**", r -> StpUtil.checkPermission("admin"));
       SaRouter.match("/goods/**", r -> StpUtil.checkPermission("goods"));
       SaRouter.match("/orders/**", r -> StpUtil.checkPermission("orders"));
       SaRouter.match("/notice/**", r -> StpUtil.checkPermission("notice"));
       // 更多模块... 
   })).addPathPatterns("/**");
   ```

2. 登录认证与单点登录

   用户认证登录的时候只在 auth 这个模块去做，根据用户 id 登录之后，利用 sa_token 生成 token 凭证，将 token -> userid 保存到 redis，然后将 token 返回到前端。前端以后再访问的时候把这个 token 带过来（可以利用 cookie、请求体或者请求参数都可以），后端可以在 filter、interceptor 或者 controller 里基于 token 去拿到 userid，然后就当做这次操作的 userid 去做后续的所有操作。

   不让前端直接传用户 ID，就是防止有人用 userid 很轻易地去模拟请求进行攻击。

   可以把 token 理解为 userid 的代理。项目里需要用到 userid 的都是利用前端传来的 token 去 redis 拿到 userid 来用的。

3. 分布式 session 会话（整合 Redis）

   微服务这种分布式环境是不同集群部署的，每个模块的 session 是不通的，Sa-Token 提供了扩展接口可以去集成 Redis，将登录信息集中式存储在 Redis 中。

   用户登录时，将 userid 与一个 session 绑定，并将 userid -> session 保存到 redis，然后就可以把用户信息塞进去。

   session 本身类似一个 map，key 必须是 String，而 value 可以是任意数据类型，如果是集合或者对象，存储到 redis 会被序列化为 JSON 字符串存储。

   不过项目里基本上没有利用分布式 session 去缓存用户信息（只有网关鉴权那里 sa-token 从 session 取用户的权限信息和角色信息），而是利用 JetCache 做二级缓存。

   > HttpSession 是基于 HTTP 协议的会话管理机制。SaSession 与 HttpSession 没有任何关系。

<u>**cookie**</u>

Cookie 是服务器发送到用户浏览器并保存在客户端本地的一小段数据，浏览器会在后续请求中自动将 Cookie 发送回服务器。

以下是一个典型的 HTTP 响应头中的 `Set-Cookie` 示例：

```yaml
Set-Cookie: session_id=abc123; Domain=.example.com; Path=/; Expires=Wed, 31 Dec 2025 23:59:59 GMT; Secure; HttpOnly; SameSite=Lax
```

一个 Cookie 通常包含：一个键值对、域、路径、过期时间等。

<u>**httpsession**</u>

Session 是一种服务器端机制，用于在多个请求之间保存用户状态。

当客户端访问服务器时，如果客户端是第一次访问服务器且请求中不包含有效的 Session ID，再加上服务器程序显式调用 `request.getSession(true)`，服务器会生成一块内存区域，该内存区域和当前客户端对应。接下来当前客户端需要存取的数据都在这块内存空间进行，实现了数据的共享。然后生成唯一 Session ID，通过响应头将 Session ID 返回客户端：

```yaml
Set-Cookie: JSESSIONID=abc123; Path=/; HttpOnly
```

客户端在后续请求中会通过 cookie 或者 url 携带该 ID，服务器据此识别用户并维护会话状态。

Session 的数据结构类似于一个 Map，本质上是一个键值对的集合。键是一个字符串，值可以是任意类型的数据，如字符串、数字、对象等。

Session 存储在服务器的内存中，也可以持久化到数据库、文件系统。服务器关闭，非持久化 Session 会丢失。浏览器关闭，会使得未持久化的 Cookie 丢失，这可能会造成保存有 JSESSIONID 的 Cookie 丢失，进而无法找到对应的 session。

Tomcat 默认 session 的超时时间是 30min，也可以显式设置。客户端每次请求都会刷新 Session 的最后访问时间。服务器程序也可以显式失效。

**<u>Filter vs HandlerInterceptor</u>**

过滤器 Filter 和拦截器 HandlerInterceptor 都是用于在请求处理过程中执行额外逻辑的组件，但它们的实现方式、作用范围和适用场景有所不同。以下是它们的详细区别：

1. 所属技术栈

   Filter 属于 Servlet 规范，是 Java EE 的一部分。可以在任何基于 Servlet 的 Web 应用中使用（如 Spring MVC、纯 Servlet 应用等）。

   HandlerInterceptor 属于 Spring MVC 框架，是 Spring 提供的一种机制，只能在 Spring MVC 应用中使用。

2. 执行位置

   Filter：在 Servlet 容器级别执行，执行顺序在 DispatcherServlet 之前。

   过滤器可以拦截所有的请求，包括静态资源（如 CSS、JS、图片等）。

   HandlerInterceptor：在 Spring MVC 框架级别 执行。拦截器的执行顺序在 DispatcherServlet 之后，但在控制器（Controller）方法执行之前。拦截器只能拦截 Spring MVC 处理的请求（即经过 DispatcherServlet 的请求），无法拦截静态资源。

3. 生命周期

   Filter 由 Servlet 容器管理。生命周期与 Servlet 容器一致，在应用启动时初始化，在应用关闭时销毁。

   HandlerInterceptor：由 Spring 容器管理。生命周期与 Spring Bean 一致，在 Spring 上下文初始化时创建，在上下文关闭时销毁。

4. 适用场景

   Filter：字符编码设置、跨域请求处理（CORS）、全局日志记录、请求和响应的修改（如压缩、加密）。

   HandlerInterceptor：权限验证、日志记录（记录控制器方法的执行时间）、修改模型数据（如添加全局属性）、请求预处理（如参数校验）。

### 基于 Redis 和 Caffeine 构建多级缓存机制，将用户信息存储到缓存中，提升查询效率

在项目中经常会需要查询用户信息，而这些用户信息，很多内容都是不变的，比如姓名、网点 id、网点名称、手机号等，或者说是变化不频发的。所以这些数据我们就可以做缓存。

在缓存选择上，不仅用了分布式缓存，同时为了提升性能，还做了本地缓存。

也就是说我们的一次查询，会先查询本地缓存，如果本地缓存查不到，再查询分布式缓存，分布式查不到，再去查数据库。后面命中的会将数据更新到前面所有缓存，并重置过期时间。

这些可以在那些查询信息的方法里手动实现，但是为了方便，减少这段代码，我们选择用通用的二级缓存框架，阿里开源的 JetCache。

只需要在那些查询信息的方法上加上 @Cached 注解，设置好 key、过期时间、刷新时间等即可。并且在修改信息的方法加了 @CacheInvalidate 注解，让本地缓存和远程缓存失效。

```c++
@Cached(name = "userCache", key = "#userId", expire = 60, refresh = 30, cacheType = CacheType.BOTH)
```

本地缓存本身就是一种牺牲一致性提升性能的方案。

**<u>CAP 理论</u>**

+ C 一致性：访问所有节点得到的结果是一致的
+ A 可用性：每个请求可以在合理时间内得到响应。
+ P 分区容错性：分区容错性要求系统在网络被割裂时仍然能继续提供服务。 

CAP 理论，指的是在一个分布式系统中因为分区容忍性的存在，就必定需要在可用性和数据一致性中做出权衡。

### 参数管理

业务对应很多凭证，每个凭证上又有很多要素，这些要素又分别对应不同的数据来源和校验规则，业务还要绑定业务规则。

## NTF

### 设计并实现订单防重复提交功能

在很多秒杀场景中，用户为了能下单成功，会频繁的点击下单按钮，这时候如果没有做好控制的话，就可能会给一个用户创建重复订单。

简单来说是使用了 token 的机制，这个 token 是用户访问页面的时候获取到的。然后再真正下单的时候再把这个 token 给删除，确保他只能用一次。

具体来说，我们定义一个接口专门用来生成 ，在下单页面渲染的时候调我们的接口去获取一下就行了。token 其实就是 UUID，生成之后先存到 redis 中（30 min），key 是 前缀 + token + 字符串 scene（前端传入，下单是 "orderList"，因为还有支付也要防重），然后再返回给前端。

前端在拿到这个 token 后，需要在下单接口中把这个 token 通过请求头（Authorization）带过来，然后我们在后端判断一下 token 的有效性。 token 的校验我们是通过 Filter 实现的（只拦截特定的 Url /trade/buy），这样做更加通用一些。

主要实现在 doFilter 方法中，主要是判断请求中是否携带了 token，如果携带了，通过 redis 校验 token 是否有效（存在），如果有效，则把这个 token 删除，并且放过请求。如果无效，则直接拒绝请求。

这里的 token 校验及移除，我们是通过 lua 脚本实现的，保证原子性。

### 基于责任链模式，在下单环节进行多维度的前置校验，提升代码的可扩展性

总的 Validator 接口 OrderCreateValidator，定义两个方法：validate 方法用于做请求检验、setNext 设置下一个校验器。

下面又做了一个实现 OrderCreateValidator 接口的抽象类 BaseOrderCreateValidator，对 validate 校验方法制定了步骤：先执行自身校验方法的具体实现，然后如果设置了下一个校验器，再去执行下一个校验器的 validate 方法。

三个具体的校验器（继承抽象类）：

1）用户校验器 UserValidator，判断用户状态是否正常以及是否做了实名认证。

虽然网关统一鉴权校验过，但是对于订单模块它自己还是有一份职责，去对下单行为做一次校验。

2）库存校验器 StockValidator，主要校验库存情况。远程调用库存微服务，去 redis 缓存中查询实时库存，库存不足就抛异常。

3）商品校验器，主要校验商品可售情况。远程调用商品服务，查询商品的状态，以及商品当前价格是否与下单时的单价是否一致。

责任链模式（Chain of Responsibility Pattern）是一种行为设计模式，可以通过将一系列处理器按顺序链接起来，使得每个处理器都有机会处理请求，从而实现请求的传递和处理。

好处：后续有新的校验，比如做了一些限购、只能预约用户下单、黑名单之类的，增加相应的 validator 只需要注册进去串起来，不需要改原来的代码，开闭原则，可扩展性好。

```java
public class UserValidator extends BaseOrderCreateValidator {

    private UserFacadeService userFacadeService;

    @Override
    public void doValidate(OrderCreateRequest request) throws OrderException {
        String buyerId = request.getBuyerId();
        ...
    }

    public UserValidator(UserFacadeService userFacadeService) {
        this.userFacadeService = userFacadeService;
    }

    public UserValidator() {
    }
}
@Configuration
public class OrderClientConfiguration {

    @Bean
    public WorkerIdHolder workerIdHolder(RedissonClient redisson) {
        return new WorkerIdHolder(redisson);
    }

    @Bean
    public GoodsValidator goodsValidator(GoodsFacadeService goodsFacadeService) {
        return new GoodsValidator(goodsFacadeService);
    }

    @Bean
    public StockValidator stockValidator(InventoryFacadeService inventoryFacadeService) {
        return new StockValidator(inventoryFacadeService);
    }

    @Bean
    public UserValidator userValidator(UserFacadeService userFacadeService) {
        return new UserValidator(userFacadeService);
    }

    @Bean
    public GoodsBookValidator goodsBookValidator(GoodsFacadeService goodsFacadeService) {
        return new GoodsBookValidator(goodsFacadeService);
    }
}
@Configuration
public class OrderPreValidatorConfig {

    @Autowired
    private UserValidator userValidator;

    @Autowired
    private GoodsValidator goodsValidator;

    @Autowired
    private StockValidator stockValidator;

    @Bean
    public OrderCreateValidator orderPreValidatorChain() {
        userValidator.setNext(goodsValidator);
        goodsValidator.setNext(stockValidator);
        return userValidator;
    }
}
```

### 基于 Lua+Redis 实现库存的高并发扣减

我做的库存扣减，是基于 Redis+数据库的，先在 Redis 中做预扣减，然后再在数据库中做扣减。

之所以在 Redis 当中做预占用，是因为数据库抗不了那么高的热点行的并发，因为在秒杀场景中，大家购买的是同一件商品，更新的是同一商品的库存，在数据库中就是频繁地去更新同一行，而不管是 mysql 和 oracle 都会在 update 的时候会去给行加悲观锁，加了这个锁就会有死锁检测以及锁的自旋，导致数据库就会在这上面不断地耗费。想想要是有 500 个事务去同时改同一条记录，就会有大量的锁冲突，导致数据库扛不住，CPU 就会拉高。所以就引入了 Redis，因为 Redis 本身是基于内存并且是单线程的，天然地排队去在内存中去执行的，效率性能很好。假如说一个商品我在 Redis 当中设置了 100 个库存，在控制好扣减的原子性的情况下，我就能确保只有 100 个线程能执行成功，超过 100 个之后的线程就会在 redis 这层失败掉。所以对于秒杀场景来说的话，相当于假如说我的库存只有 100 个，我有 10 万个人来下单，可能这 10 万流量可能都靠 redis 去扛，但是对于我的应用或者是对于我后端的数据库来讲，我其实不需要扛这么高的并发，我只需要在 redis 当中放过 100 个请求，然后我只需要数据库扛着 100 个请求就好了。

然后其实在 redis 当中都可以不用扛那么高的并发。当我 redis 当中的商品的库存变成 0 了之后，我同时去写到本地缓存当中，就是每一台机器上面会有一个本地缓存，在本地缓存去设置一下这个商品已经售罄这样的一个标识，在去 redis 扣减库存之前，先去本地缓存检查这个商品有没有售罄标识，然后 fail-fast，这样的话 redis 也不需要扛那么高的并发。不过同时还带来缓存一致性的问题，后面如果这个商品我再开卖了，或者是我再加库存了，我怎么让多个实例的本地缓存能同时的去生效，这个是一个比较麻烦和恶心的问题。所以暂时这个项目里对于这个售罄标识的本地缓存只设置了 1 分钟的有效期。

redis 单线程 + lua 脚本原子性 → 强行排队，应对高并发场景。

**<u>redis 库存</u>**

关于 redis 库存，redis 中有两个缓存，一个是库存本身的缓存，另一个是库存操作流水记录。

库存操作流水记录基于 hash 结构，一个键值对就是一条记录，其中 key 是用买家 id + 下单的 token + 库存变化量拼接到一起的，value 中记录操作类型、操作前后的库存值、变化量、操作标识（幂等号）以及操作时间戳。

lua 脚本（原子）

1. 根据下单的 token 从库存流水记录检查操作是否已经执行过。
2. 扣减库存（有可能失败，库存不存在、库存不够，抛相应的异常）
3. 增加一条库存扣减流水记录缓存（使用 hash 结构），记录操作类型、操作前后的库存值、变化量、操作标识（幂等号）、操作时间戳。

### 基于 RocketMQ 事务消息实现缓存和数据库的数据一致性

利用 RocketMQ，使得在 Redis 扣减成功后，数据库也能进行扣减，并且能够在数据库扣减失败后重试。

学习 408 进程管理那节就有学过几种进程通信方式：共享内存、消息传递、管道通信，消息队列就属于消息传递方式，随着分布式和微服务系统的发展，消息队列在系统设计中有了更大的发挥空间，使用消息队列可以降低系统耦合性、实现任务异步、有效地进行流量削峰，是分布式和微服务系统中重要的组件之一。

RocketMQ 事务消息是 RocketMQ 提供的一种分布式事务解决方案，它允许应用在分布式系统中实现 "最终一致性" 的事务模型。

核心是两阶段提交（2PC，Two-Phase Commit），将一个事务的提交过程分为两个阶段：准备阶段、提交/回滚阶段。

在基于责任链模式进行下单时的各种前置校验之后，订单服务生产一条半消息发送到 RocketMQ。半消息是一种特殊消息，对消费者不可见。在生产者成功发送半消息后，其指定的事务监听器就会去执行本地事务，也就是去 redis 上做库存扣减。根据本地事务的执行结果，决定提交或回滚半消息。在本地事务执行成功并提交后，RocketMQ 会将半消息转换为正式消息，投递给下游事务（数据库的库存扣减）。

+ 半消息发送失败或者本地事务执行失败，直接抛订单创建异常即可。
+ 正式消息下游事务消费失败（抛异常），RocketMQ 会通过重试机制确保消息最终投递成功，直到成功或达到最大重试次数。如果消息达到最大重试次数仍未消费成功，RocketMQ 会将其放入消费者的死信队列中。死信队列中的消息就需要人工干预处理。
+ 如果生产者执行本地事务时未明确返回事务状态（如崩溃），RocketMQ 会定期回查事务状态（默认每分钟回查一次，最多 15 次）。生产者需要在事务监听器中处理事务状态回查，根据本地事务的实际状态决定是提交、回滚事务还是继续回查（实现也很简单，就是去 Redis 中查询是否有扣减流水，如果有，则说明扣减成功了。如果没有，这说明没扣减成功）。
+ 下游事务通过调用订单服务，完成数据库中的商品库存的扣减、增加库存操作流水、增加一条新的订单、增加订单操作流水。

**<u>取消订单</u>**

取消订单同样是基于 RocketMQ 事务消息。

之前的创建订单，本地事务去 redis 上库存扣减以及记录库存操作流水，下游事务除了进行数据库中商品库存的扣减，还增加库存操作流水、增加一条新的订单、增加订单操作流水。

而到了取消订单这里，因为取消订单并发度不高，所以这里基于职责单一原则，将库存和订单服务拆开来，本地事务是去将数据库订单记录更新为取消以及增加订单操作流水，再由下游服务去增加数据库和 redis 的商品库存以及库存操作的流水。
